/*
Highly Optimized Object-oriented Many-particle Dynamics -- Blue Edition
(HOOMD-blue) Open Source Software License Copyright 2009-2014 The Regents of
the University of Michigan All rights reserved.

HOOMD-blue may contain modifications ("Contributions") provided, and to which
copyright is held, by various Contributors who have granted The Regents of the
University of Michigan the right to modify and/or distribute such Contributions.

You may redistribute, use, and create derivate works of HOOMD-blue, in source
and binary forms, provided you abide by the following conditions:

* Redistributions of source code must retain the above copyright notice, this
list of conditions, and the following disclaimer both in the code and
prominently in any materials provided with the distribution.

* Redistributions in binary form must reproduce the above copyright notice, this
list of conditions, and the following disclaimer in the documentation and/or
other materials provided with the distribution.

* All publications and presentations based on HOOMD-blue, including any reports
or published results obtained, in whole or in part, with HOOMD-blue, will
acknowledge its use according to the terms posted at the time of submission on:
http://codeblue.umich.edu/hoomd-blue/citations.html

* Any electronic documents citing HOOMD-Blue will link to the HOOMD-Blue website:
http://codeblue.umich.edu/hoomd-blue/

* Apart from the above required attributions, neither the name of the copyright
holder nor the names of HOOMD-blue's contributors may be used to endorse or
promote products derived from this software without specific prior written
permission.

Disclaimer

THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDER AND CONTRIBUTORS ``AS IS'' AND
ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, AND/OR ANY
WARRANTIES THAT THIS SOFTWARE IS FREE OF INFRINGEMENT ARE DISCLAIMED.

IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT,
INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING,
BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE
OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF
ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
*/

/*! \page page_mpi MPI domain decomposition

Table of contents:
 - \ref sec_mpi_overview
 - \ref sec_mpi_compilation
 - \ref sec_mpi_usage
 - \ref sec_mpi_best_practices
 - \ref sec_mpi_troubleshooting
 - \ref sec_mpi_features
<hr>


\section sec_mpi_overview Overview

HOOMD-blue supports multi-GPU (and multi-CPU) simulations using MPI. It uses
a spatial domain decomposition approach similar to the one used by LAMMPS.
Every GPU is assigned a sub-domain of the simulation box, the dimensions of which
are calculated by dividing the lengths of the simulation box by the
number of processors per dimension. The product of the number of processors along
all dimensions must equal the number of processors in the MPI job. As in single-GPU
simulations, there is a one-to-one mapping between host CPU cores (MPI ranks)
and the GPUs.

Job scripts do not need to be modified to take advantage of multi-GPU execution. However,
not all features are available in MPI mode. The list of single-GPU only features can be found below.

\section sec_mpi_compilation Compilation

For detailed compilation instructions, see
\link page_compile_guide_linux_generic here.\endlink

Compilation flags pertinent to MPI simulations are

- \b ENABLE_MPI (to enable multi-GPU simulations, must be set to \b ON)
- \b ENABLE_MPI_CUDA (optional, enables CUDA-aware MPI library support, see below)

RPM and DEB packages are built with MPI enabled. Some linux distributions do not install `mpirun` (or `mpiexec`) to
a standard `bin` directory. You may need to run `/usr/lib64/openmpi/bin/mpirun` or add `/usr/lib64/openmpi/bin` to
your `PATH`.

\section sec_mpi_usage Usage

To execute a hoomd job script on multiple GPUs, run
\code
mpirun -n 8 hoomd script.py --mode=gpu
\endcode
which will execute HOOMD on 8 processors. HOOMD automatically detects which GPUs are available and assigns them to MPI
ranks.  The syntax and name of the `mpirun` command may be different
between different MPI libraries and system architectures, check with your system documentation to find out what
launcher to use. When running on multiple nodes, the job script must be available to all nodes via a network file system.

HOOMD chooses the best spatial sub-division according to a minimum-area rule. If needed, the dimensions of the
decomposition be specified using the
<b>linear</b>, **nx**, **ny** and **nz** \link page_command_line_options command line
options\endlink. If your intention is to run HOOMD on a single GPU, you can
invoke HOOMD with no MPI launcher
~~~
hoomd script.py
~~~
instead of giving the `-n 1` argument to `mpirun`.

HOOMD-blue can also execute on many **CPU cores** in parallel.
~~~~
mpirun -n 16 hoomd script.py --mode=cpu
~~~~

\subsection sec_mpi_gpu_selec GPU selection in MPI runs

GPU selection in MPI runs is handled in one of two automatically selected ways.

1. If all GPUs in a host are set to compute exclusive mode, HOOMD-blue uses the compute exclusive mode to assign
a single rank to each GPU. In this mode, do not run more ranks per node than there are GPUs on that node or one of the
ranks will produce an error that all GPUs are in use.

2. If *not* all GPUs are in compute exclusive mode, HOOMD-blue will use information from `mpirun` to determine the
<em>local rank</em> on a node (0,1,2,... unique id for each process on a node). Each rank will use the GPU id matching
the local rank. In this mode, do not run more ranks per node than there are GPUs on that node or you will get an
error. This mode only works on clusters where scheduling is performed by node (not by core) and there are a uniform
number of GPUs on each node.

In either case, a status message is printed on startup that lists which ranks are using which GPU ids. You can use this
to verify proper GPU selection.

\section sec_mpi_best_practices Best practices

HOOMD-blue's multi-GPU performance depends on many factors, such as the model of the
actual GPU used, the type of interconnect between nodes, whether the MPI library
supports CUDA, etc. Below we list some recommendations for obtaining optimal
performance.

\subsection sec_mpi_size System size

Performance depends greatly on system size. Runs with fewer particles per GPU will execute slower due to communications
overhead. HOOMD-blue has decent strong scaling down to small numbers of particles per GPU, but to obtain high efficiency
(more than 60%) typical benchmarks need 100,000 or more particles per GPU. You should benchmark your own system of
interest with short runs to determine a reasonable range of efficient scaling behavior. Different potentials and/or
cutoff radii can greatly change scaling behavior.

\subsection sec_mpi_clusters Execution on multi-GPU clusters

When executing HOOMD-blue on multiple nodes, Infiniband (or other high-performance) interconnects are necessary to
obtain good performance.

HOOMD-blue uses shared libraries. This may cause problems on some supercomputers
(notably Cray), which expect a static executable.

\subsection sec_mpi_cluster CUDA-aware MPI libraries

The main benefit of using a CUDA-enabled MPI library is that it enables intra-node
peer-to-peer (P2P) access between several GPUs on the same PCIe bus, which increases
bandwidth. Secondarily, it may offer some additional optimization
for direct data transfer between the GPU and a network adapter.
To use these features with an MPI library that supports it,
set `ENABLE_MPI_CUDA` to **ON** for compilation.

Currently, we recommend building with `ENABLE_MPI_CUDA` **OFF**. On MPI libraries available at time of release,
enabling `ENABLE_MPI_CUDA` cuts performance in half. Systems with GPUDirect RDMA enabled improve on this somewhat,
but even on such systems typical benchmarks still run faster with `ENABLE_MPI_CUDA` **OFF**.

\subsection sec_mpi_linear Slab decomposition

For small numbers of GPUs per job (typically <= 8) that are non-prime,
the performance may be increased by using a slab decomposition.
A one-dimensional decomposition is enforced if the `--linear`
\link page_command_line_options command-line option\endlink is given.

\subsection sec_mpi_rbuff Neighbor list buffer length (r_buff)

The optimum value of the \b r_buff value of the neighbor list
(\link hoomd_script.pair.nlist.set_params() nlist.set_params()\endlink)
may be different between single- and multi-GPU runs. In multi-GPU
runs, the buffering length also determines the width of the ghost layer
runs and sets the size of messages exchanged between the processors.
To determine the optimum value, use \link hoomd_script.tune.r_buff() tune.r_buff()\endlink
command with the same number of MPI ranks that will be used for the production simulation.

\subsection sec_mpi_partition Running with multiple partitions (--nrank command-line option)

HOOMD-blue supports simulation of multiple independent replicas, with the same
number of GPUs per replica. To enable multi-replica mode, and to partition
the total number of ranks \b N into <b> p = N/n</b> replicas, where \b n is the
number of GPUs per replica, invoke HOOMD-blue with the <b>--nrank=n</b>
\link page_command_line_options command-line option\endlink

Inside the command script, the current partition can be queried using
\link hoomd_script.comm.get_partition() comm.get_partition()\endlink .

\section sec_mpi_troubleshooting Troubleshooting

- <b>My simulation does not run significantly faster on two GPUs compared to
   one GPU.</b><br>
   This is expected.  HOOMD uses internal optimizations for single-GPU runs, which
   means that there is no overhead due to MPI calls. The communication overhead can be
   20-25\% of the total performance, and is only incurred when running on more
   than one GPU.

- <b> I get a message saying "Invalid bond" </b><br>
   In multi-GPU simulations, there is a restriction on the maximal length of a single
   bond. A bond cannot be longer than half the local domain size. If this happens,
   an error is thrown. The problem can be fixed by running HOOMD on fewer
   processors, or with a larger box size.

- <b> Simulations with large numbers of nodes are slow. </b><br>
   In simulations involving many nodes, collective MPI calls can take a significant portion
   of the run time. To find out if these are limiting you, run the simulation with
   the <b>profile=True</b> option to the \link hoomd_script.run() run\endlink command.
   One reason for slow performance can be the distance check, which, by default,
   is applied every step to check if the neighbor list needs to be rebuild. It requires
   synchronization between all MPI ranks and is therefore slow.
   See \link hoomd_script.pair.nlist nlist \endlink on how to increase
   the interval (\b check_period) between distance checks, to improve performance.

\section sec_mpi_features Unsupported features in MPI

These features are not currently supported in MPI execution

- charge.pppm
- constrain.sphere
- dump.bin
- dump.mol2
- dump.pdb
- Rigid bodies
- integrate.mode_minimize_fire
- integrate.berendsen
- wall.lj

Unsupported features may be made available in later releases.
*/
